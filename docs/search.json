[
  {
    "objectID": "ST503 Quarto Doc.html",
    "href": "ST503 Quarto Doc.html",
    "title": "ST503 Course Notes",
    "section": "",
    "text": "Week 1\nOur goal is to predict a response variable, call it \\(y\\) (for example, the average velocity of a pitcher). To do so, we have predictive variables \\(x_1, x_2, \\dots\\) (for example, the height of the player, the weight of a player, etc.). To find the relationship between the response and predictors, we ideally have many different observations (many different players we observe). Loosely speaking, our goal in creating linear models is to find the coefficients \\(\\beta_i\\) to the predictors \\(x_i\\) such that the error terms \\(\\epsilon_i\\) in the equation \\(y_i=\\beta_0+\\beta_1f(x_1)+\\beta_2g(x_2)+\\dots+\\epsilon_i\\) are minimized. The term ‘linear’ in ‘linear models’ refers to the fact that the aforementioned equation is linear in the \\(\\beta_i\\). When writing out each of our observations, we see that the system of equations can be written compactly in the matrix form \\(y=X\\beta+\\epsilon\\).\n\nMatrix Review\n\nA matrix \\(A\\) is positive definite if \\(x^TAx&gt;0\\) for all non-zero \\(x\\).\nThe inner product of a vector with itself is \\(x^Tx=\\sum x_i^2\\).\nThe transpose rules for matrices \\(A,B\\) and vectors \\(x\\) are as follows:\n\n\n\n\\((A+B)^T=A^T+B^T\\)\n\\((AB)^T=B^TA^T\\)\n\\((xA)^T=xA^T\\)\n\\((Ax)^T=x^TA^T\\)\n\n\n\nIf a matrix \\(A\\) in invertible, then \\(A^TA\\) is also invertible. Thus when solving the system \\(Ax=b\\) for full-rank \\(A\\), we know \\(x=A^{-1}b=(A^TA)^{-1}A^Tb\\)\n\\(\\text{Cov}(Ay)=A \\text{Cov}(y)A^T\\)\n\\(\\text{Cov}(X,Y)=\\mathbb{E}(XY)-\\mathbb{E}(X)\\mathbb{E}(Y)\\). Since \\(\\mathbb{V}(X)=\\mathbb{E}(X^2)-\\mathbb{E}(X)^2\\), \\(\\mathbb{V}(X)=\\text{Cov}(X,X)\\). If random variables are independent, then covariance is zero.\nThe covariance matrix of a vector \\(\\epsilon\\) is as expected: \\(\\text{Cov}(\\epsilon)=\\begin{bmatrix}\n\\mathbb{V}(\\epsilon_1) & \\text{Cov}(\\epsilon_1, \\epsilon_2) & \\dots &  \\text{Cov}(\\epsilon_1, \\epsilon_n)\\\\ &\\mathbb{V}(\\epsilon_2) & \\dots &\\text{Cov}(\\epsilon_2, \\epsilon_n) \\\\\n\\\\ & & \\ddots &\\vdots \\\\ & & & \\mathbb{V}(\\epsilon_n) \\end{bmatrix}\\)\nThe correlation \\(\\rho\\) between variables \\(X\\) and \\(Y\\) is \\(\\frac{\\text{Cov}(X,Y)}{\\sqrt{\\mathbb{V}(X)\\mathbb{V}(Y)}}\\)\n\n\n\nHandy R Code\n\nWe can find the transpose of a matrix with the R code t(a). For example:\n\n\na=matrix(c(1,3,2,4), nrow=2, byrow=TRUE)\na\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nt(a)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\n\nWe can perform matrix multiplication with `%*%’. For example:\n\n\nb=matrix(1:4, nrow=2, byrow=TRUE)\na %*% b\n\n     [,1] [,2]\n[1,]   10   14\n[2,]   14   20\n\n\n\nWe can find the inverse of a matrix with solve. For example:\n\n\nsolve(a)\n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\na %*% solve(a)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\n\nWe can find the rank of a matrix with qr (for the QR decomposition). For example:\n\n\nqr(a)     #qr(a)$rank gives 2#\n\n$qr\n           [,1]       [,2]\n[1,] -2.2360680 -4.9193496\n[2,]  0.8944272 -0.8944272\n\n$rank\n[1] 2\n\n$qraux\n[1] 1.4472136 0.8944272\n\n$pivot\n[1] 1 2\n\nattr(,\"class\")\n[1] \"qr\"\n\n\n\n\nClasses of Linear Models\n\nLeast-squares: \\(y=X\\beta+\\epsilon\\)\nGauss-Markov: \\(y=X\\beta+\\epsilon\\) where \\(\\mathbb{E}(\\epsilon)=0\\) and \\(\\text{Cov}(\\epsilon)=\\sigma^2 I\\) (i.e. there is no covariance between epsilon terms).\nAitken Model: \\(y=X\\beta+\\epsilon\\) where \\(\\mathbb{E}(\\epsilon)=0\\) and \\(\\text{Cov}(\\epsilon)=\\sigma^2 V\\) for some known positive definite matrix \\(V\\)\nOne-Way ANOVA. Interested in if \\(\\mu_1=\\mu_2\\). We can write \\(\\mu_i=\\mu+(\\mu_i-\\mu)=\\mu+\\alpha_i\\). Then \\(\\begin{bmatrix}y_{1_1} \\\\ \\vdots \\\\ y_{1_{n}} \\\\ y_{2_1} \\\\ \\vdots \\\\ y_{2_n}\\end{bmatrix}=\\begin{bmatrix} 1 & 0 \\\\ \\vdots &\\vdots \\\\ 1 & 0 \\\\ 0 & 1 \\\\ \\vdots & \\vdots \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} \\mu_1 \\\\ \\mu_2\\end{bmatrix}+\\begin{bmatrix} \\epsilon_{1_1} \\\\ \\vdots \\\\ \\epsilon_{1_{n}} \\\\ \\epsilon_{2_1} \\\\ \\vdots \\\\ \\epsilon_{2_n}\\end{bmatrix}\\)\n\n\n\nExample\nWe have data on the gambling habits of teenagers in England.\n\nlibrary(faraway)\nsummary(teengamb)\n\n      sex             status          income           verbal     \n Min.   :0.0000   Min.   :18.00   Min.   : 0.600   Min.   : 1.00  \n 1st Qu.:0.0000   1st Qu.:28.00   1st Qu.: 2.000   1st Qu.: 6.00  \n Median :0.0000   Median :43.00   Median : 3.250   Median : 7.00  \n Mean   :0.4043   Mean   :45.23   Mean   : 4.642   Mean   : 6.66  \n 3rd Qu.:1.0000   3rd Qu.:61.50   3rd Qu.: 6.210   3rd Qu.: 8.00  \n Max.   :1.0000   Max.   :75.00   Max.   :15.000   Max.   :10.00  \n     gamble     \n Min.   :  0.0  \n 1st Qu.:  1.1  \n Median :  6.0  \n Mean   : 19.3  \n 3rd Qu.: 19.4  \n Max.   :156.0  \n\nstr(teengamb)\n\n'data.frame':   47 obs. of  5 variables:\n $ sex   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ status: int  51 28 37 28 65 61 28 27 43 18 ...\n $ income: num  2 2.5 2 7 2 3.47 5.5 6.42 2 6 ...\n $ verbal: int  8 8 6 4 8 6 7 5 6 7 ...\n $ gamble: num  0 0 0 7.3 19.6 0.1 1.45 6.6 1.7 0.1 ...\n\n\nWe can try to predict the annual amount gambled by their weekly income.\n\ngamble_regression=lm(gamble ~ income, data=teengamb)\nsummary(gamble_regression)\n\n\nCall:\nlm(formula = gamble ~ income, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.020 -11.874  -3.757  11.934 107.120 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -6.325      6.030  -1.049      0.3    \nincome         5.520      1.036   5.330 3.05e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.95 on 45 degrees of freedom\nMultiple R-squared:  0.387, Adjusted R-squared:  0.3734 \nF-statistic: 28.41 on 1 and 45 DF,  p-value: 3.045e-06\n\n\nIn such a simple model, we can just read off the estimates for \\(\\beta_0\\) (the intercept) and \\(\\beta_1\\) the coefficient for weekly income. We can also access them directly with the following:\n\ngambling_coef=gamble_regression$coefficients\n\nAll told, our model is \\(\\begin{bmatrix}y_1\\\\ \\vdots \\\\y_n \\end{bmatrix}=\\begin{bmatrix}1 & x_1\\\\ \\vdots &\\vdots\\\\1&x_n \\end{bmatrix} \\begin{bmatrix} -6.352\\\\5.52\\end{bmatrix}+\\begin{bmatrix}\\epsilon_1\\\\ \\vdots\\\\ \\epsilon_n \\end{bmatrix}\\)"
  }
]