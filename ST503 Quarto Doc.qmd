---
title: "ST503 Course Notes"
format: html
editor: visual
---

# Week 1

Our goal is to predict a response variable, call it $y$ (for example, the average velocity of a pitcher). To do so, we have predictive (covariate) variables $x_1, x_2, \dots$ (for example, the height of the player, the weight of a player, etc.). To find the relationship between the response and predictors, we ideally have many different observations (many different players we observe). Loosely speaking, our goal in creating linear models is to find the coefficients $\beta_i$ to the predictors $x_i$ such that the error terms $\epsilon_i$ in the equation $y_i=\beta_0+\beta_1f(x_1)+\beta_2g(x_2)+\dots+\epsilon_i$ are minimized. The term 'linear' in 'linear models' refers to the fact that the aforementioned equation is linear *in the* $\beta_i$. When writing out each of our observations, we see that the system of equations can be written compactly in the matrix form $y=X\beta+\epsilon$.

### Matrix Review

1.  A matrix $A$ is positive definite if $x^TAx>0$ for all non-zero $x$.
2.  The inner product of a vector with itself is $x^Tx=\sum x_i^2$.
3.  The transpose rules for matrices $A,B$ and vectors $x$ are as follows:

<!-- -->

a.  $(A+B)^T=A^T+B^T$
b.  $(AB)^T=B^TA^T$
c.  $(xA)^T=xA^T$
d.  $(Ax)^T=x^TA^T$
e.  \$(A^{-1})^T=(A^T)^{-1}

<!-- -->

4.  If a matrix $A$ in invertible, then $A^TA$ is also invertible. Thus when solving the system $Ax=b$ for full-rank $A$, we know $x=A^{-1}b=(A^TA)^{-1}A^Tb$
5.  $\text{Cov}(Ay)=A \text{Cov}(y)A^T$ and $\mathbb{V}(A^Ty)=A^T \text{Cov}(y)A$.
6.  $\text{Cov}(X,Y)=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)$. Since $\mathbb{V}(X)=\mathbb{E}(X^2)-\mathbb{E}(X)^2$, $\mathbb{V}(X)=\text{Cov}(X,X)$. If random variables are independent, then covariance is zero.
7.  The covariance matrix of a vector $\epsilon$ is as expected: $\text{Cov}(\epsilon)=\begin{bmatrix}
    \mathbb{V}(\epsilon_1) & \text{Cov}(\epsilon_1, \epsilon_2) & \dots &  \text{Cov}(\epsilon_1, \epsilon_n)\\ &\mathbb{V}(\epsilon_2) & \dots &\text{Cov}(\epsilon_2, \epsilon_n) \\
    \\ & & \ddots &\vdots \\ & & & \mathbb{V}(\epsilon_n) \end{bmatrix}$
8.  The correlation $\rho$ between variables $X$ and $Y$ is $\frac{\text{Cov}(X,Y)}{\sqrt{\mathbb{V}(X)\mathbb{V}(Y)}}$
9.  The projection of a vector $y$ onto the column space of $A$ is $y^*=P_Ay$ where $P_A$ is the projection matrix $A(A^TA)^{-1}A^T$.
10. Projection matrices are symmetric ($P_A=P_A^T$) and idempotent ($P_X^2=P_X$).
11. The norm of a vector $x$ is $\|x\|=\langle x, x \rangle=(x_1^2+\dots+x_n^2)^{\frac{1}{2}}$.
12. Differentiating a scalar $v$ with respect to a vector $x$ returns a vector whose components are the partial derivatives with respect to each component of $x$, $\frac{d v}{dx}=\begin{bmatrix} \frac{\partial v}{\partial x_1} &\dots & \frac{\partial v}{\partial x_n}\end{bmatrix}^T$. This is called the **gradient**
13. Differentiating a vector $x\in \mathbb{R}^n$ with respect to another vector $y \in \mathbb{R}^m$ returns an $m \times n$ matrix of partials called the **Jacobian Matrix**, $\frac{d x}{dy}=\begin{bmatrix} \frac{\partial x_1}{\partial y_1} & \dots &\frac{\partial x_n}{\partial y_1}\\ \vdots &\ddots & \vdots \\ \frac{\partial x_1}{\partial y_m} & \dots &\frac{\partial x_m}{\partial y_m}
    \end{bmatrix}$
14. In the case where $x,y \in \mathbb{R}^n$, then $\frac{d x^Ty}{d y}=x$
15. In the case where $x\in \mathbb{R}^n$ and $A \in \mathbb{R}^{n \times n}$, then $\frac{d x^TAx}{d x}=(A+A^T)x$

### Handy R Code

1.  We can find the transpose of a matrix with the `R` code `t(a)`. For example:

```{r}
a=matrix(c(1,3,2,4), nrow=2, byrow=TRUE)
a
t(a)
```

2.  We can perform matrix multiplication with \`%\*%'. For example:

```{r}
b=matrix(1:4, nrow=2, byrow=TRUE)
a %*% b

```

3.  We can find the inverse of a matrix with `solve`. For example:

```{r}
solve(a)
a %*% solve(a)
```

4.  We can find the rank of a matrix with `qr` (for the QR decomposition). For example:

```{r}
qr(a)     #qr(a)$rank gives 2#
```

### Classes of Linear Models

1.  Least-squares: $y=X\beta+\epsilon$
2.  Gauss-Markov: $y=X\beta+\epsilon$ where $\mathbb{E}(\epsilon)=0$ and $\text{Cov}(\epsilon)=\sigma^2 I$ (i.e. there is no covariance between epsilon terms).
3.  Aitken Model: $y=X\beta+\epsilon$ where $\mathbb{E}(\epsilon)=0$ and $\text{Cov}(\epsilon)=\sigma^2 V$ for some known positive definite matrix $V$
4.  One-Way ANOVA. Interested in if $\mu_1=\mu_2$. We can write $\mu_i=\mu+(\mu_i-\mu)=\mu+\alpha_i$. Then $\begin{bmatrix}y_{1_1} \\ \vdots \\ y_{1_{n}} \\ y_{2_1} \\ \vdots \\ y_{2_n}\end{bmatrix}=\begin{bmatrix} 1 & 0 \\ \vdots &\vdots \\ 1 & 0 \\ 1 & 1 \\ \vdots & \vdots \\ 1 & 1 \end{bmatrix} \begin{bmatrix} \mu \\ \alpha_1\end{bmatrix}+\begin{bmatrix} \epsilon_{1_1} \\ \vdots \\ \epsilon_{1_{n}} \\ \epsilon_{2_1} \\ \vdots \\ \epsilon_{2_n}\end{bmatrix}$

### Example

We have data on the gambling habits of teenagers in England.

```{r}
library(faraway)
summary(teengamb)
str(teengamb)
```

We can try to predict the annual amount gambled by their weekly income.

```{r}
gamble_regression=lm(gamble ~ income, data=teengamb)
summary(gamble_regression)
```

In such a simple model, we can just read off the estimates for $\beta_0$ (the intercept) and $\beta_1$ the coefficient for weekly income. We can also access them directly with the following:

```{r}
gambling_coef=gamble_regression$coefficients
gambling_coef
```

All told, our model is $\begin{bmatrix}y_1\\ \vdots \\y_n \end{bmatrix}=\begin{bmatrix}1 & x_1\\ \vdots &\vdots\\1&x_n \end{bmatrix} \begin{bmatrix} -6.352\\5.52\end{bmatrix}+\begin{bmatrix}\epsilon_1\\ \vdots\\ \epsilon_n \end{bmatrix}$

# Week 2

Our model is $y=X\beta+\epsilon$. Our goal is to choose $\beta$ such that $\|\epsilon\|^2=\|y-X\beta\|^2$ is minimized. Expanding the inner product, we have $(y-X\beta)^T(y-X\beta)=y^Ty-2y^TX\beta+\beta^TX^TX\beta$. Since $y$ is determined, it suffices to minimize $f(\beta)=\beta^TX^TX\beta-2y^TX\beta$. From matrix review steps 11-14, we have $\frac{d f}{d \beta}=2X^TX \beta-2X^Ty$, and so the equation is minimized when $X^TX\beta=X^Ty$. This is called the **normal equation** (normal because the columns of $X$ are normal to $y-X\beta)$. When $X$ is invertible, this implies that our **ordinary least squares solution** is $\hat{\beta}=(X^TX)^{-1}X^T y$.

Note that our model $y=X\beta+\epsilon$ can be written $y=v+\epsilon$ where $v \in C(X)$ (that is what it means to be in the column space). Thus we are choosing the $v \in C(X)$ that is closest to $y$. Of course, this is the orthogonal projection of $y$ onto $C(X)$. From step 9, our estimate for $v$ is then $P_Xy=X(X^TX)^{-1}y$ and so we have $X\hat{\beta}=X(X^TX)^{-1}y$. Multiplying both sides on the left by $X^T$, we get back our normal equation, $X^TX\beta=X^Ty$, and (when $X$ is invertible), find our predicted values of our response to be $\hat{y}=X\hat{\beta}=X(X^TX)^{-1}X^T y=P_Xy$. The residuals of our model are the observed values $y$ less the predicted values $\hat{y}=X\hat{\beta}$; $\hat{e}=y-P_Xy=(I-P_X)y$.

Does our ordinaly least squares solution (OLS) $\hat{\beta}$ have any nice properties in a Gauss-Markov model? For one, it is unbiased: $\mathbb{E}(\hat{\beta})=\beta$. See that $\mathbb{E}(\hat{\beta})=\mathbb{E}\left((X^TX)^{-1}X^Ty\right)=(X^TX)^{-1}X^T\mathbb{E}(y)$, and then by the Gauss-Markov assumption that $\mathbb{E}(\epsilon)=0$, we can continue $(X^TX)^{-1}X^T\left(\mathbb{E}(X\beta)+\mathbb{E}(\epsilon)\right)=(X^TX)^{-1}X^TX\beta=\beta$. Using our 5th fact from the matrix review, we further know that $\text{Cov}(\hat{\beta})=\text{Cov}((X^TX)^{-1}X^Ty)=\left[(X^TX)^{-1}X^T\right]\text{Cov}(y)\left[(X^TX)^{-1}X^T\right]^T$, and by the Gauss-Markov assumption that $\text{Cov}(\epsilon)=\sigma^2I$, we can continue (after pulling out the scalar $\sigma^2$) as $\sigma^2\left[(X^TX)^{-1}X^T\right]\left[(X^TX)^{-1}X^T\right]^T=\sigma^2\left[(X^TX)^{-1}X^T\right]\left[X((X^TX)^T)^{-1}\right]=\sigma^2\left[(X^TX)^{-1}X^T\right]\left[X(X^TX)^{-1}\right]$ (from matrix review 3b and 3e). Simplifying terms, we see that $\text{Cov}(\hat{\beta})=\sigma^2(X^TX)^{-1}$.

See slides 16-19 for why $\text{Cov}(\hat{\epsilon})=\sigma^2(I-P_X)=\mathbb{E}(\hat{\epsilon}\hat{\epsilon}^T)=\sigma^2(n-rank(X))$, thus $\frac{\hat{\epsilon}^T\hat{\epsilon}}{n-\text{rank}(X)}$ is an unbiased estimator of $\sigma^2$. The **sum Of square errors (SSE)**, also called **residual sum of squares (RSS)**, is the inner product of $\hat{\epsilon}$ with itself, $SSE=\hat{\epsilon}^T\hat{\epsilon}$. So our unbiased estimate for $\sigma^2$ is $\hat{\sigma^2}=\frac{SSE}{n-\text{rank}(X)}$.

In the general case, we may not have invertible $X$. In such cases, the normal equation is still $X^TX\beta=X^Ty$, but we now use a **generalized inverse (g-inverse)** to come up with our estimate. A matrix $G$ is called a g-inverse of a matrix $A$ if and only if $AGA=A$, and every matrix $A$ has a (non-unique) g-inverse. So while our predicted response for models where $X$ is not invertible is $\hat{y}=X\hat{\beta}$ where $\hat{\beta}=(X^TX)^-X^Ty$, even though $(X^TX)^-$ is not unique, $X(X^TX)^-X^T$ is (no matter the choice of $\hat{\beta}$, $\hat{y}$ is unique). The residuals and estimator of $\sigma^2$ are also unique.

### Comparing R Estimates To Derived Estimates

Our ordinary least-squares solution for a Gauss-Markov model was shown to be $\hat{\beta}=(X^TX)^{-1}X^T y$. Let's verify that the regression coefficients we get from R match these values.

```{r}
library(faraway)
gamble_regression=lm(gamble ~ ., data=teengamb) #all variables predictors#
X=model.matrix(gamble~., data=teengamb) 
y=teengamb$gamble

bhat=solve(t(X) %*% X) %*% t(X) %*%y
compare_coef=data.frame(direct=bhat, lm=gamble_regression$coefficients)
compare_coef

```

Great, now that we've shown that our estimate $\hat{\beta}$ matches up, we may be interested in the standard error of our estimate. We first need to compute our estimate of $\hat{sigma^2}$, which we showed earlier was $\frac{\hat{\epsilon}^T\hat{\epsilon}}{n-\text{rank}(X)}$. We verify our calculations match with the following.

```{r}
res=gamble_regression$residual #observed less predicted#
SSE=sum(res^2)
n=nrow(X)                      #X is the model matrix defined above#
r=qr(X)$rank
sigmasqrhat=SSE/(n-r)
sigmahat=sqrt(sigmasqrhat)

compare_sigma=data.frame(direct=sigmahat, lm=summary(gamble_regression)$sigma)
compare_sigma

```

With our estimate $\hat{\sigma^2}$ in hand, we can compute the standard error of our estimates of the regression coefficients. We have previously derived that $\text{Cov}(\hat{\beta})=\hat{\sigma^2}(X^TX)^{-1}$. The standard error of any element of $\hat{\beta}$ is found from the diagonal element of the covariance matrix (square root since the diagonal of the covariance matrix are the variances); $\text{SE}(\beta_i)=\hat{\sigma}\sqrt{(X^TX)^{-1}_{i,i}}$. We finally verify these match R with the following:

```{r}
sigmahat                                      #from above code chunk#
beta_se=sigmahat*sqrt(diag(solve(t(X)%*%X)))  #X from previous code chunk#

compare_se=data.frame(direct=beta_se, 
                      lm=summary(gamble_regression)$coef[, "Std. Error"])
compare_se
```

Note that while we could grab our regression coefficients $\hat{\beta}$ with `gamble_regression$coefficients`, our residual standard error with `summary(gamble_regression)$sigma`, and our standard errors with `summary(gamble_regression)$coef[, "Std. Error"]`, we can also read these directly off the summary from the first column, third to last row, and second column respectively.

```{r}
summary(gamble_regression)
```

### Estimability

What values can be estimated with an unbiased estimator in a Gauss-Markov model? In essence, we are asking which **linera functions** of $\beta$, call them $c^T\beta$ for some vector $c$, are estimable. A **linear estimator** has the form $l^Ty$ for some vector $l$. Then the function $c^T\beta$ is **estimable** if and only if there is a linear estimator $l^Ty$ such that $\mathbb{E}(l^Ty)=c^T\beta$ for all $\beta$. Such estimators $l^Ty$ are called a **linear unbiased estimator (LUE)** of $c^T\beta$. 

Suppose $c^T\beta$ is estimable. Then $\mathbb{E}(l^Ty)=c^T\beta \implies l^TX\beta=c^T\beta \implies l^TX=c^T$. In other words, $c^T\beta$ is estimable if and only if $c$ is in the column space of $X^T$ (the row space of $X$). Since the left null space is orthogonal to the row space, $c^T\beta$ is also estimable if and only if $C$ is orthogonal to the left null space.

For an example, recall our ANOVA model: $\begin{bmatrix}y_{1_1} \\ \vdots \\ y_{1_{n}} \\ y_{2_1} \\ \vdots \\ y_{2_n} \\ y_{3_1} \\ \vdots \\ y_{3_n}\end{bmatrix}=\begin{bmatrix} 1 & 1 &0  \\ \vdots &\vdots &\vdots \\ 1 & 1 &0 \\ 1 & 0&1 \\ \vdots & \vdots &\vdots \\ 1 & 0 &1\end{bmatrix} \begin{bmatrix} \mu \\ \alpha_1 \\ \alpha_2\end{bmatrix}+\begin{bmatrix} \epsilon_{1_1} \\ \vdots \\ \epsilon_{1_{n}} \\ \epsilon_{2_1} \\ \vdots \\ \epsilon_{2_n} \\ \epsilon_{3_1} \\ \vdots \\ \epsilon_{3_n}\end{bmatrix}$. Note that $\mu+\alpha_1=\begin{bmatrix} 1&1&0\end{bmatrix}\begin{bmatrix} \mu \\ \alpha_1 \\ \alpha_2\end{bmatrix}=c^T\beta$ is estimable, since $c$ is in the row space of $X$ (it is the very first row). On the other hand, $\mu=\begin{bmatrix} 1&0&0\end{bmatrix}\begin{bmatrix} \mu \\ \alpha_1 \\ \alpha_2\end{bmatrix}$ is not estimable, since the row space of $X$ is $\text{span}\left\{\begin{pmatrix}1\\1\\0\end{pmatrix}^T, \begin{pmatrix}0\\0\\1\end{pmatrix}^T\right\}$. 

If $c^T\beta$ is estimable, then $c^T\hat{\beta}$ is a LUE. linear since $c^T\hat{\beta}=c^T(C^TC)^-C^Ty=l^Ty$. Unbiased since $\mathbb{E}(c^T\hat{\beta})=\mathbb{E}(l^TX\hat{\beta})=\mathbb{E}(l^T\hat{y})=l^T\mathbb{E}(\hat{y})=l^TX\beta=c^T\beta$. Look at pages 6 and 7 on week 3 A to see $\mathbb{V}(c^T\beta)=c^T\sigma^2(X^TX)^-c$.



### Checking estimability with R

There is a package `estimability` in R that helps determine the estimability of linear function of $\beta$. 

```{r}
library(estimability)

X=cbind(rep(1,5), 1:5, 5:1, 2:6)
cvec1=c(1,4,2,5)        #so, c^T\beta=1\beta_0+4\beta_1+...#
nb=nonest.basis(X)     #some basis vectors of left null space#
is.estble(cvec1, nb)

cvec2=c(1,2,2,1)
is.estble(cvec2, nb)

```


The key here is the `is.estble` function, which requires we come up with the left null space via the `nonest.basis` function. We can see that the basis generated by this function is indeed a basis for the left null space by direct computation (that the values aren't exactly zero is a storage/rounding error)

```{r}
nb
t(X)[,1] %*% nb  #zero with computer rounding#
```

Likewise we see that the first linear function we tested, $c^T\beta=\beta_0+4\beta_1+2\beta_2+5\beta_3$, is indeed estimable, since it is orthogonal to the left null space of our model matrix $X$:

```{r}
t(cvec1) %*% nb   #zero with computer rounding#
```

On the otherhand, the second linear function we tested, $\beta_0+2\beta_1+2\beta_2+\beta_3$ is not estimable, since it isn't orthogonal to the left null space of $X$.

```{r}
t(cvec2) %*% nb
```