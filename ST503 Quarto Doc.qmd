---
title: "ST503 Course Notes"
format: html
editor: visual
---

# Week 1

Our goal is to predict a response variable, call it $y$ (for example, the average velocity of a pitcher). To do so, we have predictive (covariate) variables $x_1, x_2, \dots$ (for example, the height of the player, the weight of a player, etc.). To find the relationship between the response and predictors, we ideally have many different observations (many different players we observe). Loosely speaking, our goal in creating linear models is to find the coefficients $\beta_i$ to the predictors $x_i$ such that the error terms $\epsilon_i$ in the equation $y_i=\beta_0+\beta_1f(x_1)+\beta_2g(x_2)+\dots+\epsilon_i$ are minimized. The term 'linear' in 'linear models' refers to the fact that the aforementioned equation is linear *in the* $\beta_i$. When writing out each of our observations, we see that the system of equations can be written compactly in the matrix form $y=X\beta+\epsilon$.

### Matrix Review

1.  A matrix $A$ is positive definite if $x^TAx>0$ for all non-zero $x$.
2.  The inner product of a vector with itself is $x^Tx=\sum x_i^2$.
3.  The transpose rules for matrices $A,B$ and vectors $x$ are as follows:

<!-- -->

a.  $(A+B)^T=A^T+B^T$
b.  $(AB)^T=B^TA^T$
c.  $(xA)^T=xA^T$
d.  $(Ax)^T=x^TA^T$
e.  $(A^{-1})^T=(A^T)^{-1}

<!-- -->

4.  If a matrix $A$ in invertible, then $A^TA$ is also invertible. Thus when solving the system $Ax=b$ for full-rank $A$, we know $x=A^{-1}b=(A^TA)^{-1}A^Tb$
5.  $\text{Cov}(Ay)=A \text{Cov}(y)A^T$
6.  $\text{Cov}(X,Y)=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)$. Since $\mathbb{V}(X)=\mathbb{E}(X^2)-\mathbb{E}(X)^2$, $\mathbb{V}(X)=\text{Cov}(X,X)$. If random variables are independent, then covariance is zero.
7.  The covariance matrix of a vector $\epsilon$ is as expected: $\text{Cov}(\epsilon)=\begin{bmatrix}
    \mathbb{V}(\epsilon_1) & \text{Cov}(\epsilon_1, \epsilon_2) & \dots &  \text{Cov}(\epsilon_1, \epsilon_n)\\ &\mathbb{V}(\epsilon_2) & \dots &\text{Cov}(\epsilon_2, \epsilon_n) \\
    \\ & & \ddots &\vdots \\ & & & \mathbb{V}(\epsilon_n) \end{bmatrix}$
8.  The correlation $\rho$ between variables $X$ and $Y$ is $\frac{\text{Cov}(X,Y)}{\sqrt{\mathbb{V}(X)\mathbb{V}(Y)}}$
9.  The projection of a vector $y$ onto the column space of $A$ is $y^*=P_Ay$ where $P_A$ is the projection matrix $A(A^TA)^{-1}A^T$.
10. Projection matrices are symmetric ($P_A=P_A^T$) and idempotent ($P_X^2=P_X$).
11. The norm of a vector $x$ is $\|x\|=\langle x, x \rangle=(x_1^2+\dots+x_n^2)^{\frac{1}{2}}$.
12. Differentiating a scalar $v$ with respect to a vector $x$ returns a vector whose components are the partial derivatives with respect to each component of $x$, $\frac{d v}{dx}=\begin{bmatrix} \frac{\partial v}{\partial x_1} &\dots & \frac{\partial v}{\partial x_n}\end{bmatrix}^T$. This is called the **gradient**
13. Differentiating a vector $x\in \mathbb{R}^n$ with respect to another vector $y \in \mathbb{R}^m$ returns an $m \times n$ matrix of partials called the **Jacobian Matrix**, $\frac{d x}{dy}=\begin{bmatrix} \frac{\partial x_1}{\partial y_1} & \dots &\frac{\partial x_n}{\partial y_1}\\ \vdots &\ddots & \vdots \\ \frac{\partial x_1}{\partial y_m} & \dots &\frac{\partial x_m}{\partial y_m}
    \end{bmatrix}$
14. In the case where $x,y \in \mathbb{R}^n$, then $\frac{d x^Ty}{d y}=x$
15. In the case where $x\in \mathbb{R}^n$ and $A \in \mathbb{R}^{n \times n}$, then $\frac{d x^TAx}{d x}=(A+A^T)x$

### Handy R Code

1.  We can find the transpose of a matrix with the `R` code `t(a)`. For example:

```{r}
a=matrix(c(1,3,2,4), nrow=2, byrow=TRUE)
a
t(a)
```

2.  We can perform matrix multiplication with \`%\*%'. For example:

```{r}
b=matrix(1:4, nrow=2, byrow=TRUE)
a %*% b

```

3.  We can find the inverse of a matrix with `solve`. For example:

```{r}
solve(a)
a %*% solve(a)
```

4.  We can find the rank of a matrix with `qr` (for the QR decomposition). For example:

```{r}
qr(a)     #qr(a)$rank gives 2#
```

### Classes of Linear Models

1.  Least-squares: $y=X\beta+\epsilon$
2.  Gauss-Markov: $y=X\beta+\epsilon$ where $\mathbb{E}(\epsilon)=0$ and $\text{Cov}(\epsilon)=\sigma^2 I$ (i.e. there is no covariance between epsilon terms).
3.  Aitken Model: $y=X\beta+\epsilon$ where $\mathbb{E}(\epsilon)=0$ and $\text{Cov}(\epsilon)=\sigma^2 V$ for some known positive definite matrix $V$
4.  One-Way ANOVA. Interested in if $\mu_1=\mu_2$. We can write $\mu_i=\mu+(\mu_i-\mu)=\mu+\alpha_i$. Then $\begin{bmatrix}y_{1_1} \\ \vdots \\ y_{1_{n}} \\ y_{2_1} \\ \vdots \\ y_{2_n}\end{bmatrix}=\begin{bmatrix} 1 & 0 \\ \vdots &\vdots \\ 1 & 0 \\ 0 & 1 \\ \vdots & \vdots \\ 0 & 1 \end{bmatrix} \begin{bmatrix} \mu_1 \\ \mu_2\end{bmatrix}+\begin{bmatrix} \epsilon_{1_1} \\ \vdots \\ \epsilon_{1_{n}} \\ \epsilon_{2_1} \\ \vdots \\ \epsilon_{2_n}\end{bmatrix}$

### Example

We have data on the gambling habits of teenagers in England.

```{r}
library(faraway)
summary(teengamb)
str(teengamb)
```

We can try to predict the annual amount gambled by their weekly income.

```{r}
gamble_regression=lm(gamble ~ income, data=teengamb)
summary(gamble_regression)
```

In such a simple model, we can just read off the estimates for $\beta_0$ (the intercept) and $\beta_1$ the coefficient for weekly income. We can also access them directly with the following:

```{r}
gambling_coef=gamble_regression$coefficients
gambling_coef
```

All told, our model is $\begin{bmatrix}y_1\\ \vdots \\y_n \end{bmatrix}=\begin{bmatrix}1 & x_1\\ \vdots &\vdots\\1&x_n \end{bmatrix} \begin{bmatrix} -6.352\\5.52\end{bmatrix}+\begin{bmatrix}\epsilon_1\\ \vdots\\ \epsilon_n \end{bmatrix}$

# Week 2

Our model is $y=X\beta+\epsilon$. Our goal is to choose $\beta$ such that $\|\epsilon\|^2=\|y-X\beta\|^2$ is minimized. Expanding the inner product, we have $(y-X\beta)^T(y-X\beta)=y^Ty-2y^TX\beta+\beta^TX^TX\beta$. Since $y$ is determined, it suffices to minimize $f(\beta)=\beta^TX^TX\beta-2y^TX\beta$. From matrix review steps 11-14, we have $\frac{d f}{d \beta}=2X^TX \beta-2X^Ty$, and so the equation is minimized when $X^TX\beta=X^Ty$. This is called the **normal equation** (normal because the columns of $X$ are normal to $y-X\beta)$. When $X$ is invertible, this implies that our **ordinary least squares solution** is $\hat{\beta}=(X^TX)^{-1}X^T y$.

Note that our model $y=X\beta+\epsilon$ can be written $y=v+\epsilon$ where $v \in C(X)$ (that is what it means to be in the column space). Thus we are choosing the $v \in C(X)$ that is closest to $y$. Of course, this is the orthogonal projection of $y$ onto $C(X)$. From step 9, our estimate for $v$ is then $P_Xy=X(X^TX)^{-1}y$ and so we have $X\hat{\beta}=X(X^TX)^{-1}y$. Multiplying both sides on the left by $X^T$, we get back our normal equation, $X^TX\beta=X^Ty$, and (when $X$ is invertible), find our predicted values of our response to be $\hat{y}=X\hat{\beta}=X(X^TX)^{-1}X^T y=P_Xy$. The residuals of our model are the observed values $y$ less the predicted values $\hat{y}=X\hat{\beta}$; $\hat{e}=y-P_Xy=(I-P_X)y$.

Does our ordinaly least squares solution (OLS) $\hat{\beta}$ have any nice properties in a Gauss-Markov model? For one, it is unbiased: $\mathbb{E}(\hat{\beta})=\beta$. See that $\mathbb{E}(\hat{\beta})=\mathbb{E}\left((X^TX)^{-1}X^Ty\right)=(X^TX)^{-1}X^T\mathbb{E}(y)$, and then by the Gauss-Markov assumption that $\mathbb{E}(\epsilon)=0$, we can continue $(X^TX)^{-1}X^T\left(\mathbb{E}(X\beta)+\mathbb{E}(\epsilon)\right)=(X^TX)^{-1}X^TX\beta=\beta$. Using our 5th fact from the matrix review, we further know that $\text{Cov}(\hat{\beta})=\text{Cov}((X^TX)^{-1}X^Ty)=\left[(X^TX)^{-1}X^T\right]\text{Cov}(y)\left[(X^TX)^{-1}X^T\right]^T$, and by the Gauss-Markov assumption that $\text{Cov}(\epsilon)=\sigma^2I$, we can continue (after pulling out the scalar $\sigma^2$) as $\sigma^2\left[(X^TX)^{-1}X^T\right]\left[(X^TX)^{-1}X^T\right]^T=\sigma^2\left[(X^TX)^{-1}X^T\right]\left[X((X^TX)^T)^{-1}\right]=\sigma^2\left[(X^TX)^{-1}X^T\right]\left[X(X^TX)^{-1}\right]$ (from matrix review 3b and 3e). Simplifying terms, we see that $\text{Cov}(\hat{\beta})=\sigma^2(X^TX)^{-1}$.

See slides 16-19 for why $\text{Cov}(\hat{\epsilon})=\sigma^2(I-P_X)=\mathbb{E}(\hat{\epsilon}\hat{\epsilon}^T)=\sigma^2(n-rank(X))$, thus $\frac{\hat{\epsilon}^T\hat{\epsilon}}{n-\text{rank}(X)}$ is an unbiased estimator of $\sigma^2$. The **sum Of square errors (SSE)**, also called **residual sum of squares (RSS)**, is the inner product of $\hat{\epsilon}$ with itself, $SSE=\hat{\epsilon}^T\hat{\epsilon}$. So our unbiased estimate for $\sigma^2$ is $\hat{\sigma^2}=\frac{SSE}{n-\text{rank}(X)}$.

In the general case, we may not have invertible $X$. In such cases, the normal equation is still $X^TX\beta=X^Ty$, but we now use a **generalized inverse (g-inverse)** to come up with our estimate. A matrix $G$ is called a g-inverse of a matrix $A$ if and only if $AGA=A$, and every matrix $A$ has a (non-unique) g-inverse. So while our predicted response for models where $X$ is not invertible is $\hat{y}=X\hat{\beta}$ where $\hat{\beta}=(X^TX)^-X^Ty$, even though $(X^TX)^-$ is not unique, $X(X^TX)^-X^T$ is (no matter the choice of $\hat{\beta}$, $\hat{y}$ is unique). The residuals and estimator of $\sigma^2$ are also unique.


